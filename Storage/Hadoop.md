Apache Hadoop HDFS: The Hadoop Distributed File System (HDFS) is designed to store very large data sets reliably, and to stream those data sets at high bandwidth to user applications.


Link: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html

Distributed: HDFS is a distributed file system designed to run on commodity hardware. It is designed with fault tolerance in mind and can handle very large datasets.

Big-Data: HDFS is designed to handle big data. It can store and process large volumes of data in parallel, making it a key component of many big data solutions.

Batch: HDFS is particularly well-suited to batch processing. It's often used with MapReduce, a computational model that allows for processing and generating large datasets in a batch manner.

