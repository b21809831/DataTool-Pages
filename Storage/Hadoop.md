Apache Hadoop HDFS
The Hadoop Distributed File System (HDFS) is designed to store very large data sets reliably, and to stream those data sets at high bandwidth to user applications.


Link: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html

Scalable: HDFS is designed to reliably store very large files across machines in a large cluster.

Lightly processed: HDFS exposes a file system namespace and allows user data to be stored in files. It is designed to support large files, suitable for applications with big data sets, where the data is lightly processed.


Distributed: HDFS is a distributed file system designed to run on commodity hardware. It is designed with fault tolerance in mind and can handle very large datasets.

Big-Data: HDFS is designed to handle big data. It can store and process large volumes of data in parallel, making it a key component of many big data solutions.

Batch: HDFS is particularly well-suited to batch processing. It's often used with MapReduce, a computational model that allows for processing and generating large datasets in a batch manner.

